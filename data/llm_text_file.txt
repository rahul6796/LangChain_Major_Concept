{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue26;\red255\green255\blue255;\red39\green78\blue192;
}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c13333;\cssrgb\c100000\c100000\c100000;\cssrgb\c20000\c40000\c80000;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww30040\viewh16140\viewkind0
\deftab720
\pard\pardeftab720\sa320\partightenfactor0

\f0\fs32 \cf2 \cb3 \expnd0\expndtw0\kerning0
A\'a0
\f1\b large language model
\f0\b0 \'a0(
\f1\b LLM
\f0\b0 ) is a computational\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Model#Conceptual_model"}}{\fldrslt \cf4 model}}\'a0notable for its ability to achieve general-purpose language generation and other\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Natural_language_processing"}}{\fldrslt \cf4 natural language processing}}\'a0tasks such as\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Statistical_classification"}}{\fldrslt \cf4 classification}}. Based on\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Language_model"}}{\fldrslt \cf4 language models}}, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a computationally intensive\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Self-supervised_learning"}}{\fldrslt \cf4 self-supervised}}\'a0and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Semi-supervised_learning"}}{\fldrslt \cf4 semi-supervised}}\'a0training process.{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Large_language_model#cite_note-:7-1"}}{\fldrslt 
\fs25\fsmilli12800 \cf4 \super [1]}}\'a0LLMs can be used for text generation, a form of\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Generative_artificial_intelligence"}}{\fldrslt \cf4 generative AI}}, by taking an input text and repeatedly predicting the next token or word.{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Large_language_model#cite_note-Bowman-2"}}{\fldrslt 
\fs25\fsmilli12800 \cf4 \super [2]}}\
LLMs are\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Artificial_neural_network"}}{\fldrslt \cf4 artificial neural networks}}\'a0that utilize the\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"}}{\fldrslt \cf4 transformer}}\'a0architecture, invented in 2017. The largest and most capable LLMs, as of June\'a02024, are built with a decoder-only transformer-based architecture, which enables efficient processing and generation of large-scale text data.\
Historically, up to 2020,\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)"}}{\fldrslt \cf4 fine-tuning}}\'a0was the primary method used to adapt a model for specific tasks. However, larger models such as\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/GPT-3"}}{\fldrslt \cf4 GPT-3}}\'a0have demonstrated the ability to achieve similar results through\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Prompt_engineering"}}{\fldrslt \cf4 prompt engineering}}, which involves crafting specific input prompts to guide the model's responses.{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Large_language_model#cite_note-few-shot-learners-3"}}{\fldrslt 
\fs25\fsmilli12800 \cf4 \super [3]}}\'a0These models acquire knowledge about syntax, semantics, and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Ontology_(information_science)"}}{\fldrslt \cf4 ontologies}}{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Large_language_model#cite_note-4"}}{\fldrslt 
\fs25\fsmilli12800 \cf4 \super [4]}}\'a0inherent in human language corpora, but they also inherit inaccuracies and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Algorithmic_bias"}}{\fldrslt \cf4 biases}}\'a0present in the data they are trained on.{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Large_language_model#cite_note-Manning-2022-5"}}{\fldrslt 
\fs25\fsmilli12800 \cf4 \super [5]}}\
Some notable LLMs are\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/OpenAI"}}{\fldrslt \cf4 OpenAI}}'s\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer"}}{\fldrslt \cf4 GPT}}\'a0series of models (e.g.,\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/GPT-3.5"}}{\fldrslt \cf4 GPT-3.5}},\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/GPT-4"}}{\fldrslt \cf4 GPT-4}}\'a0and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/GPT-4o"}}{\fldrslt \cf4 GPT-4o}}; used in\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/ChatGPT"}}{\fldrslt \cf4 ChatGPT}}\'a0and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Microsoft_Copilot"}}{\fldrslt \cf4 Microsoft Copilot}}),\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Google"}}{\fldrslt \cf4 Google}}'s\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Gemini_(language_model)"}}{\fldrslt \cf4 Gemini}}\'a0(the latter of which is currently used in\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Gemini_(chatbot)"}}{\fldrslt \cf4 the chatbot of the same name}}),\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Meta_Platforms"}}{\fldrslt \cf4 Meta}}'s\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/LLaMA"}}{\fldrslt \cf4 LLaMA}}\'a0family of models,\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Anthropic"}}{\fldrslt \cf4 Anthropic}}'s\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Claude_(language_model)"}}{\fldrslt \cf4 Claude}}\'a0models, and\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Mistral_AI"}}{\fldrslt \cf4 Mistral AI}}'s models.\
}